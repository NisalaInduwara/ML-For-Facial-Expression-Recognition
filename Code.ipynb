{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1be29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ebe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining image folders paths\n",
    "TRAIN_IMAGE = 'D:\\\\acedemic folders\\\\6th semester\\\\projects\\\\ML-For-Facial-Expression-Recognition\\\\train'\n",
    "VALIDATION_IMAGE = 'D:\\\\acedemic folders\\\\6th semester\\\\projects\\\\ML-For-Facial-Expression-Recognition\\\\validation'\n",
    "\n",
    "LR = 0.001 #This variable represents the learning rate for the model's optimizer.\n",
    "BATCH_SIZE = 32 #This variable represents the number of images in each training batch.\n",
    "EPOCHS = 25 #This variable represents the number of times the model will iterate over the entire training dataset.\n",
    "\n",
    "DEVICE = 'cuda' #This variable specifies the device on which the model will be trained.\n",
    "MODEL_NAME = 'efficientnet_b0' #This variable represents the name or identifier of the model architecture that will be used for facial expression recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ba5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to implement pytorch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f6ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable represents a sequence of image augmentation transformations to be applied to the training images.\n",
    "train_aug = T.Compose([ # The T.Compose() function is used to combine multiple transformations into a single pipeline\n",
    "    T.RandomHorizontalFlip(p = 0.5), #  This transformation randomly flips the image horizontally with a probability of 0.5. It helps the model to generalize by considering both original and horizontally flipped images during training.\n",
    "    T.RandomRotation(degrees = (-20, +20)), # This transformation randomly rotates the image by a degree within the range of -20 to +20. It introduces variability and allows the model to learn from slightly rotated images, enhancing its ability to recognize facial expressions from different angles.\n",
    "    T.ToTensor() # This transformation converts the augmented image to a tensor. \n",
    "])\n",
    "\n",
    "valid_aug = T.Compose([ # valid_aug variable represents a sequence of image augmentation transformations to be applied to the validation images.\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Overall, the image augmentation transformations defined in train_aug and valid_aug help introduce variations and increase the diversity of the training data while maintaining the original image format. This augmentation process can improve the model's ability to generalize and perform well on unseen data by exposing it to different orientations and perspectives of facial expressions during training.\n",
    "# how the index of image change when transform from numpy arr into torch tensor\n",
    "# numpy arr (h, w, c) -> (c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c39cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageFolder(TRAIN_IMAGE, transform=train_aug)\n",
    "validset = ImageFolder(VALIDATION_IMAGE, transform=valid_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b66bf493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of examples in trainset : 28821\n",
      "Total no. of examples in validset : 7066\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total no. of examples in trainset : {len(trainset)}\")\n",
    "print(f\"Total no. of examples in validset : {len(validset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20db24be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(trainset.class_to_idx)\n",
    "\n",
    "# The code print(trainset.class_to_idx) is used to display the mapping between class labels and their corresponding indices in the training dataset (trainset). When working with image classification tasks, the class_to_idx attribute of the dataset provides a dictionary that maps class labels to numerical indices.\n",
    "#In the code snippet, trainset.class_to_idx is printed, which will output the dictionary containing the class-to-index mapping for the training dataset. Each class label is represented as a key in the dictionary, and its corresponding index is the value associated with that key.\n",
    "#For example, if you have three classes in your training dataset: \"happy\", \"sad\", and \"angry\", the class_to_idx dictionary may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dfc26c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting the image and the label from the trainset\n",
    "image, label = trainset[6000]\n",
    "# p;ot the image using permutation\n",
    "plt.imshow(image.permute(1, 2, 0))#(h, w, c)\n",
    "plt.title(label);\n",
    "\n",
    "# the shape (C, H, W), where C represents the number of channels, H represents the height, and W represents the width of the image. However, the plt.imshow() function expects the image to have the shape (H, W, C) to display it correctly.\n",
    "# The image.permute(1, 2, 0) operation rearranges the dimensions of the tensor according to the provided permutation (1, 2, 0). This means that the second dimension of the tensor will become the first dimension, the third dimension will become the second dimension, and the original first dimension will become the third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e0211c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader # Data loaders provide an efficient way to load and iterate over the dataset in mini-batches during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5209406",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True) # trainloader is created using the DataLoader class. It takes the trainset dataset as input and specifies the batch size (BATCH_SIZE) and the shuffle parameter set to True. The shuffle=True argument means that the training dataset will be randomly shuffled during the training process, which helps in preventing the model from learning the order of the samples.\n",
    "validloader = DataLoader(validset, batch_size=BATCH_SIZE) # validloader is created in a similar way but does not include the shuffle parameter. By default, the shuffle parameter is set to False, which means the validation dataset will not be shuffled during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d62fe31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of batches in trainloader : 901\n",
      "Total no. of batches in validloader : 221\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total no. of batches in trainloader : {len(trainloader)}\")\n",
    "print(f\"Total no. of batches in validloader : {len(validloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86eacd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One image batch shape : torch.Size([32, 3, 48, 48])\n",
      "One label batch shape : torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in trainloader: # This loop iterates over the trainloader and retrieves the first mini-batch of images and corresponding labels. The loop is stopped after the first iteration using the break statement.\n",
    "    break;\n",
    "    \n",
    "print(f\"One image batch shape : {images.shape}\")\n",
    "print(f\"One label batch shape : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc57d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b8cb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class FaceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceModel, self).__init__()\n",
    "        \n",
    "        # Initialize the efficientnet_b0 model from the timm library\n",
    "        # with pretrained weights and 7 output classes\n",
    "        self.eff_net = timm.create_model('efficientnet_b0', pretrained=True, num_classes=7)\n",
    "        \n",
    "    def forward(self, images, labels=None):\n",
    "        # Forward pass of the model\n",
    "        \n",
    "        # Pass the images through the efficientnet_b0 model\n",
    "        logits = self.eff_net(images)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # If labels are provided, calculate the cross-entropy loss\n",
    "            # between the logits and the labels\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            return logits, loss\n",
    "        \n",
    "        # If labels are not provided, return the logits\n",
    "        return logits\n",
    "\n",
    "    \n",
    "#The purpose of this code is to define a custom face model based on the efficientnet_b0 architecture and provide the functionality to perform forward passes and calculate the loss during training. The forward() method encapsulates the computation performed by the model and enables flexibility in training and inference scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1040e160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769ef8a7d5d34202a86ab01e25a8fbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NisalaInduwara\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NisalaInduwara\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m FaceModel()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = FaceModel()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8cc2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
